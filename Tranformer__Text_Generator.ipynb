{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristianAlegreBustos/Artificial-Intelligence/blob/main/Tranformer__Text_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Automatic Text Content Generator\n",
        "\n",
        "# I. Introduction\n",
        "We train a model with the content of these author’s published works in order to generate new text content with the same style and voice as the original author.\n",
        "\n",
        "\n",
        "### Narrow circumstances\n",
        "\n",
        "We conducted this experiment using Google Colab’s free tier, which comes with notable limitations in runtime duration, storage, and compute power\n",
        "\n",
        "While Colab enabled us to prototype and explore quickly, additional compute time and budget would allow us to scale up experiments, improve model quality, and reach more coherent, author‑style outputs.\n",
        "---\n",
        "\n",
        "## II. Methodology\n",
        "For our network we used transformer layers to generate the data. These layers offer us a way to process data parallelly, increasing the performance of the training. This parallel input also allows us to process data that has differing variable lengths like we have in our training dataset.\n",
        "\n",
        "For training the model the method that we used is one called Teacher forcing. This method solves an issue during training where the model’s current output becomes its next input. If one of its inputs is wrong then this can slowly build over time until the model is producing gibberish. Teacher forcing replaces the model’s bad output with the expected output for the next time the model runs its computation. This solution is easy to deploy and leads to much faster model training, but it can come with some downsides when the final model is deployed.\n",
        "\n",
        "For judging our model’s accuracy we used logits and sparse categorical cross-entropy. The logits work as a numerical output that we can use as a predictor of likelihood for our output. This is then passed into the sparse categorical cross‑entropy function which will give us a loss output that we can use for seeing how well our model is performing.\n",
        "\n",
        "---\n",
        "\n",
        "## III. Results\n",
        "The following table summarizes the outcomes of the various training strategies and parameters and our observation of the results:\n",
        "\n",
        "| Experiment       | vocab_size | sequence_length | pretrain_epochs | finetune_epochs | Learning Rate (Fine‑tune) | Key Result                                                |\n",
        "|------------------|------------|------------------|------------------|------------------|----------------------------|-----------------------------------------------------------|\n",
        "| Baseline         | 8,192      | 128              | 25               | 3                | 1.00E‑05                   | Incoherent, repetitive words                              |\n",
        "| + More Time      | 8,192      | 128              | 100              | 15               | 1.00E‑05                   | Slightly better, author‑specific words appear             |\n",
        "| + Larger Vocab   | 16,384     | 128              | 100              | 15               | 1.00E‑05                   | Fewer [UNK] tokens, but still no structure                |\n",
        "| + Warmup         | 16,384     | 128              | 100              | 15               | 5.00E‑05                   | Improved training stability (hypothesis)                 |\n",
        "| + More Data      | 16,384     | 128              | 100              | 15               | 5.00E‑05                   | Final Recommended Configuration                           |\n",
        "\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "Below are Github Gist links to the notebooks we used during this case study:\n",
        "\n",
        "- https://colab.research.google.com/drive/1dk6aTzNnDR‑WNYTGYXnHc0Bc0mgGLHrF?authuser=1#scrollTo=XqfcwtwfBr0T\n",
        "\n",
        "**Prompt:**  \n",
        "She entered the room and was surprised to see that  \n",
        "**Text by Model:**  \n",
        "she entered the room and was surprised to see that she had not turned away, when catherine, whose terror, her face, and the mother, stood anxiously watching her sister for the next moment, cried eagerly,\" you have seen my dear, how d’ [UNK] come in here! - - how strange!\" and listening began another application of these congratulations, which, on the occasion, really expecting to be in the house, was quite [UNK]. her anxious heart beat quick and agitated as she felt and cried in a shrinking from\n",
        "\n",
        "---\n",
        "\n",
        "## IV. Discussion Responses\n",
        "Firstly, we debated between LSTM and GRU layers. GRUs train faster and need less data, but we ultimately chose transformer layers for their ability to process longer sequences in parallel, unlike the sequential nature of GRUs.\n",
        "\n",
        "Next, we addressed the issue of unintelligible output. We recommend increasing training epochs, as the model likely hasn’t learned the patterns well yet. If output doesn’t improve after more steps, consider adding layers to improve its capacity to fit the data.\n",
        "\n",
        "We also thought about how to make the project more engaging. To excite investors, we could show outputs—like interactive demos, graphs, or sample text—to highlight current performance and future potential.\n",
        "\n",
        "We compared teacher forcing with curriculum learning. Teacher forcing is quicker to implement and improves short‑term accuracy, but it can make the model dependent. Curriculum learning takes longer but helps the model learn more independently.\n",
        "\n",
        "In short, teacher forcing is faster but riskier long‑term. Curriculum learning is more complex but can yield a more reliable model with time and effort.\n",
        "\n",
        "We’re continuing to use logits with sparse categorical cross‑entropy, as used by the previous team. It provides good prediction probability and fits our current training setup well.\n",
        "\n",
        "Lastly, to boost performance, we’ll incorporate text from Wikipedia and other Creative Commons sources. These are modern, clean, and manageable, offering useful data for training.\n"
      ],
      "metadata": {
        "id": "4XIIWWFAYosm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g42ZSwKYX3o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "No9ttGaU9tO3",
        "outputId": "e2e30691-e27b-46d2-a32b-b1662130796e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
            "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.9)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m159.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m122.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, google-pasta, tensorboard, astunparse, tensorflow\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 6.31.1\n",
            "    Uninstalling protobuf-6.31.1:\n",
            "      Successfully uninstalled protobuf-6.31.1\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 protobuf-5.29.5 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "f237999eede34ee8bb8f60588f4d366e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Tranformer_starter_publishing_word_unified.ipynb\n",
        "\n",
        "Versión final del script que implementa un Transformer con una estrategia\n",
        "de tokenización por palabras y vocabulario unificado para máxima estabilidad.\n",
        "Incluye el flujo completo de pre-entrenamiento y fine-tuning.\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. IMPORTACIONES Y CONFIGURACIÓN INICIAL\n",
        "# ==============================================================================\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import StringLookup, Layer, Embedding, MultiHeadAttention, LayerNormalization, Dense, Dropout, TextVectorization\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import string\n",
        "import gc\n",
        "import random\n",
        "import contextlib\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "print(\"GPU disponible:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# --- Parámetros Globales ---\n",
        "path = ''\n",
        "vocab_size = 32000\n",
        "sequence_length = 128 # Longitud de secuencia para palabras\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. FUNCIONES DE PREPROCESAMIENTO Y OBTENCIÓN DE DATOS\n",
        "# ==============================================================================\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Limpia y pre-procesa el texto para la tokenización por palabras.\"\"\"\n",
        "    text = text.replace(\"Project Gutenberg\", \"\")\n",
        "    text = text.replace(\"Gutenberg\", \"\")\n",
        "    text = text.replace(\"\\r\", \"\")\n",
        "    text = text.replace(\"“\", \"\\\"\")\n",
        "    text = text.replace(\"”\", \"\\\"\")\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"\\t\", \" \")\n",
        "    text = re.sub(r\"(?<![a-zA-Z])([A-Z])\", lambda match: f\"^{match.group(0).lower()}\", text)\n",
        "    text = re.sub(r\"([A-Z])\", lambda match: f\"{match.group(0).lower()}\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, f\" {punctuation} \")\n",
        "    return text\n",
        "\n",
        "def postprocess_text(text):\n",
        "    \"\"\"Revierte parte del pre-procesamiento para hacer el texto legible.\"\"\"\n",
        "    text = re.sub(r\"\\^([a-z])\", lambda match: f\"{match.group(1).upper()}\", text)\n",
        "    text = text.replace(\"^\", \"\")\n",
        "    # Colapsa espacios extra que puedan quedar después de unir tokens\n",
        "    text = re.sub(r'\\s+([,.!?;\"])', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "def getMyText():\n",
        "    \"\"\"Descarga el corpus del autor específico (conan doyle).\"\"\"\n",
        "    file_name = 'conan_doyle.txt'\n",
        "    file_url = 'https://www.gutenberg.org/files/1661/1661-0.txt'\n",
        "    downloaded_path = tf.keras.utils.get_file(file_name, file_url, cache_dir='.', cache_subdir='')\n",
        "    with open(downloaded_path, 'rb') as f:\n",
        "        text = f.read().decode(encoding='utf-8')\n",
        "    return preprocess_text(text)\n",
        "\n",
        "def getRandomText(numbooks=50, verbose=False):\n",
        "    \"\"\"Descarga un corpus general de N libros de Project Gutenberg.\"\"\"\n",
        "    download_log = io.StringIO()\n",
        "    text_random = ''\n",
        "    books_found = 0\n",
        "    while books_found < numbooks:\n",
        "        booknum = random.randint(100, 70000)\n",
        "        if verbose: print(f\"Intentando libro #{booknum}...\")\n",
        "        try:\n",
        "            url = f'https://www.gutenberg.org/cache/epub/{booknum}/pg{booknum}.txt'\n",
        "            with contextlib.redirect_stdout(download_log):\n",
        "                path_to_file_temp = tf.keras.utils.get_file(f'pg{booknum}.txt', url, cache_dir='.', cache_subdir='')\n",
        "            with open(path_to_file_temp, 'rb') as f:\n",
        "                temptext = f.read().decode(encoding='utf-8')\n",
        "            if 'Language: English' in temptext and len(temptext) > 200000:\n",
        "                text_random += temptext[2000:202000]\n",
        "                books_found += 1\n",
        "                if verbose: print(f\"Libro #{booknum} descargado. {books_found}/{numbooks} libros encontrados.\")\n",
        "        except Exception:\n",
        "            continue\n",
        "    return preprocess_text(text_random)\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    return sequence[:-1], sequence[1:]\n",
        "\n",
        "def setup_dataset(dataset):\n",
        "    return dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. PREPARACIÓN DE DATOS MEJORADA (POR PALABRAS, CON VOCABULARIO UNIFICADO)\n",
        "# ==============================================================================\n",
        "print(\"--- Iniciando Preparación de Datos por Palabras (Vocabulario Unificado) ---\")\n",
        "\n",
        "# 3.1. Obtener AMBOS corpus\n",
        "print(\"Obteniendo los corpus de texto...\")\n",
        "corpus_autor = getMyText()\n",
        "corpus_general = getRandomText(numbooks=200, verbose=True)\n",
        "\n",
        "# 3.2. Crear un corpus COMBINADO para adaptar el vocabulario\n",
        "print(\"Creando corpus combinado para el vocabulario...\")\n",
        "texto_combinado = corpus_autor + \" \" + corpus_general\n",
        "\n",
        "# 3.3. Crear y adaptar la capa TextVectorization SOBRE EL TEXTO COMBINADO\n",
        "print(\"Creando y adaptando el vocabulario unificado...\")\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize='lower',\n",
        "    split='whitespace',\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int'\n",
        ")\n",
        "vectorize_layer.adapt([texto_combinado])\n",
        "vocabulary = vectorize_layer.get_vocabulary()\n",
        "del texto_combinado\n",
        "gc.collect()\n",
        "\n",
        "# 3.4. Crear los datasets separados USANDO EL MISMO vectorize_layer\n",
        "print(\"Creando datasets de pre-entrenamiento y fine-tuning...\")\n",
        "def text_to_dataset_unified(text, vector_layer):\n",
        "    all_ids = vector_layer([text])\n",
        "#Aplanar vector\n",
        "    all_ids = tf.reshape(all_ids, [-1])\n",
        "    ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "    sequences = ids_dataset.batch(sequence_length + 1, drop_remainder=True)\n",
        "    dataset = sequences.map(split_input_target)\n",
        "    return dataset\n",
        "\n",
        "dataset_preentrenamiento = setup_dataset(text_to_dataset_unified(corpus_general, vectorize_layer))\n",
        "dataset_finetuning = setup_dataset(text_to_dataset_unified(corpus_autor, vectorize_layer))\n",
        "\n",
        "del corpus_autor, corpus_general\n",
        "gc.collect()\n",
        "\n",
        "print(\"✅ Preparación por palabras unificada completada.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. DEFINICIÓN DEL MODELO TRANSFORMER\n",
        "# ==============================================================================\n",
        "def positional_encoding(length, depth):\n",
        "    depth = depth / 2\n",
        "    positions = np.arange(length)[:, np.newaxis]\n",
        "    depths = np.arange(depth)[np.newaxis, :] / depth\n",
        "    angle_rates = 1 / (10000**depths)\n",
        "    angle_rads = positions * angle_rates\n",
        "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class PositionalEmbedding(Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim, mask_zero=True)\n",
        "        self.pos_encoding = positional_encoding(length=2048, depth=embedding_dim)\n",
        "    def call(self, x):\n",
        "        length = tf.shape(x)[1]\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
        "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embedding_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
        "        self.ffn = tf.keras.Sequential([Dense(feed_forward_dim, activation='relu'), Dense(embedding_dim)])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(dropout_rate)\n",
        "        self.dropout2 = Dropout(dropout_rate)\n",
        "    def call(self, x, training=False):\n",
        "        attn_output = self.mha(query=x, value=x, key=x, use_causal_mask=True)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TransformerTextModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, feed_forward_dim, num_transformer_blocks, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = PositionalEmbedding(vocab_size, embedding_dim)\n",
        "        self.transformer_blocks = [\n",
        "            TransformerBlock(embedding_dim, num_heads, feed_forward_dim, dropout_rate)\n",
        "            for _ in range(num_transformer_blocks)\n",
        "        ]\n",
        "        self.dense_output = Dense(vocab_size)\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.embedding(inputs)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, training=training)\n",
        "        return self.dense_output(x)\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    self.d_model = tf.cast(d_model, tf.float32)\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eI3QnS-30F4z",
        "outputId": "206fba86-5056-458b-ac23-1639691cc41a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow: 2.19.0\n",
            "GPU disponible: []\n",
            "--- Iniciando Preparación de Datos por Palabras (Vocabulario Unificado) ---\n",
            "Obteniendo los corpus de texto...\n",
            "Downloading data from https://www.gutenberg.org/files/1661/1661-0.txt\n",
            "\u001b[1m607504/607504\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
            "Intentando libro #34445...\n",
            "Intentando libro #39082...\n",
            "Libro #39082 descargado. 1/200 libros encontrados.\n",
            "Intentando libro #66226...\n",
            "Intentando libro #16074...\n",
            "Libro #16074 descargado. 2/200 libros encontrados.\n",
            "Intentando libro #14238...\n",
            "Intentando libro #57020...\n",
            "Intentando libro #49773...\n",
            "Intentando libro #23063...\n",
            "Intentando libro #46675...\n",
            "Libro #46675 descargado. 3/200 libros encontrados.\n",
            "Intentando libro #59745...\n",
            "Libro #59745 descargado. 4/200 libros encontrados.\n",
            "Intentando libro #14935...\n",
            "Intentando libro #2791...\n",
            "Libro #2791 descargado. 5/200 libros encontrados.\n",
            "Intentando libro #21291...\n",
            "Libro #21291 descargado. 6/200 libros encontrados.\n",
            "Intentando libro #65695...\n",
            "Libro #65695 descargado. 7/200 libros encontrados.\n",
            "Intentando libro #34294...\n",
            "Intentando libro #14212...\n",
            "Intentando libro #65636...\n",
            "Libro #65636 descargado. 8/200 libros encontrados.\n",
            "Intentando libro #38395...\n",
            "Intentando libro #39483...\n",
            "Libro #39483 descargado. 9/200 libros encontrados.\n",
            "Intentando libro #60910...\n",
            "Intentando libro #67338...\n",
            "Libro #67338 descargado. 10/200 libros encontrados.\n",
            "Intentando libro #51753...\n",
            "Intentando libro #69025...\n",
            "Libro #69025 descargado. 11/200 libros encontrados.\n",
            "Intentando libro #50181...\n",
            "Libro #50181 descargado. 12/200 libros encontrados.\n",
            "Intentando libro #55410...\n",
            "Intentando libro #26633...\n",
            "Intentando libro #7303...\n",
            "Libro #7303 descargado. 13/200 libros encontrados.\n",
            "Intentando libro #23369...\n",
            "Intentando libro #30954...\n",
            "Libro #30954 descargado. 14/200 libros encontrados.\n",
            "Intentando libro #67520...\n",
            "Libro #67520 descargado. 15/200 libros encontrados.\n",
            "Intentando libro #56022...\n",
            "Libro #56022 descargado. 16/200 libros encontrados.\n",
            "Intentando libro #69037...\n",
            "Libro #69037 descargado. 17/200 libros encontrados.\n",
            "Intentando libro #66276...\n",
            "Libro #66276 descargado. 18/200 libros encontrados.\n",
            "Intentando libro #66875...\n",
            "Libro #66875 descargado. 19/200 libros encontrados.\n",
            "Intentando libro #5731...\n",
            "Libro #5731 descargado. 20/200 libros encontrados.\n",
            "Intentando libro #13703...\n",
            "Intentando libro #59980...\n",
            "Intentando libro #39715...\n",
            "Intentando libro #6496...\n",
            "Intentando libro #57207...\n",
            "Libro #57207 descargado. 21/200 libros encontrados.\n",
            "Intentando libro #20886...\n",
            "Intentando libro #19753...\n",
            "Libro #19753 descargado. 22/200 libros encontrados.\n",
            "Intentando libro #49744...\n",
            "Intentando libro #57607...\n",
            "Intentando libro #24965...\n",
            "Intentando libro #46986...\n",
            "Libro #46986 descargado. 23/200 libros encontrados.\n",
            "Intentando libro #15986...\n",
            "Intentando libro #46731...\n",
            "Intentando libro #63691...\n",
            "Intentando libro #8812...\n",
            "Intentando libro #11794...\n",
            "Intentando libro #46943...\n",
            "Libro #46943 descargado. 24/200 libros encontrados.\n",
            "Intentando libro #40800...\n",
            "Libro #40800 descargado. 25/200 libros encontrados.\n",
            "Intentando libro #49585...\n",
            "Intentando libro #59155...\n",
            "Intentando libro #29041...\n",
            "Libro #29041 descargado. 26/200 libros encontrados.\n",
            "Intentando libro #3713...\n",
            "Intentando libro #56170...\n",
            "Libro #56170 descargado. 27/200 libros encontrados.\n",
            "Intentando libro #45451...\n",
            "Libro #45451 descargado. 28/200 libros encontrados.\n",
            "Intentando libro #69054...\n",
            "Libro #69054 descargado. 29/200 libros encontrados.\n",
            "Intentando libro #33970...\n",
            "Libro #33970 descargado. 30/200 libros encontrados.\n",
            "Intentando libro #58008...\n",
            "Intentando libro #35243...\n",
            "Intentando libro #62621...\n",
            "Libro #62621 descargado. 31/200 libros encontrados.\n",
            "Intentando libro #1852...\n",
            "Libro #1852 descargado. 32/200 libros encontrados.\n",
            "Intentando libro #8794...\n",
            "Intentando libro #4401...\n",
            "Intentando libro #45265...\n",
            "Intentando libro #7553...\n",
            "Intentando libro #53399...\n",
            "Intentando libro #166...\n",
            "Libro #166 descargado. 33/200 libros encontrados.\n",
            "Intentando libro #8082...\n",
            "Libro #8082 descargado. 34/200 libros encontrados.\n",
            "Intentando libro #26813...\n",
            "Intentando libro #23824...\n",
            "Intentando libro #51775...\n",
            "Libro #51775 descargado. 35/200 libros encontrados.\n",
            "Intentando libro #9495...\n",
            "Intentando libro #28188...\n",
            "Libro #28188 descargado. 36/200 libros encontrados.\n",
            "Intentando libro #53505...\n",
            "Intentando libro #63368...\n",
            "Intentando libro #28996...\n",
            "Libro #28996 descargado. 37/200 libros encontrados.\n",
            "Intentando libro #60909...\n",
            "Intentando libro #61905...\n",
            "Intentando libro #2181...\n",
            "Libro #2181 descargado. 38/200 libros encontrados.\n",
            "Intentando libro #48225...\n",
            "Libro #48225 descargado. 39/200 libros encontrados.\n",
            "Intentando libro #14247...\n",
            "Intentando libro #57051...\n",
            "Intentando libro #62183...\n",
            "Intentando libro #62008...\n",
            "Intentando libro #22669...\n",
            "Libro #22669 descargado. 40/200 libros encontrados.\n",
            "Intentando libro #20926...\n",
            "Intentando libro #11755...\n",
            "Intentando libro #29742...\n",
            "Intentando libro #67483...\n",
            "Intentando libro #49494...\n",
            "Libro #49494 descargado. 41/200 libros encontrados.\n",
            "Intentando libro #30673...\n",
            "Intentando libro #1819...\n",
            "Intentando libro #64064...\n",
            "Intentando libro #12748...\n",
            "Libro #12748 descargado. 42/200 libros encontrados.\n",
            "Intentando libro #45421...\n",
            "Intentando libro #66307...\n",
            "Libro #66307 descargado. 43/200 libros encontrados.\n",
            "Intentando libro #25045...\n",
            "Intentando libro #12996...\n",
            "Intentando libro #31441...\n",
            "Intentando libro #58052...\n",
            "Intentando libro #27197...\n",
            "Libro #27197 descargado. 44/200 libros encontrados.\n",
            "Intentando libro #25381...\n",
            "Intentando libro #23543...\n",
            "Intentando libro #10217...\n",
            "Libro #10217 descargado. 45/200 libros encontrados.\n",
            "Intentando libro #14350...\n",
            "Libro #14350 descargado. 46/200 libros encontrados.\n",
            "Intentando libro #858...\n",
            "Intentando libro #40452...\n",
            "Libro #40452 descargado. 47/200 libros encontrados.\n",
            "Intentando libro #40070...\n",
            "Intentando libro #3413...\n",
            "Libro #3413 descargado. 48/200 libros encontrados.\n",
            "Intentando libro #29132...\n",
            "Intentando libro #64333...\n",
            "Libro #64333 descargado. 49/200 libros encontrados.\n",
            "Intentando libro #64497...\n",
            "Intentando libro #3061...\n",
            "Intentando libro #66315...\n",
            "Intentando libro #62471...\n",
            "Intentando libro #20216...\n",
            "Libro #20216 descargado. 50/200 libros encontrados.\n",
            "Intentando libro #40255...\n",
            "Libro #40255 descargado. 51/200 libros encontrados.\n",
            "Intentando libro #23386...\n",
            "Libro #23386 descargado. 52/200 libros encontrados.\n",
            "Intentando libro #68898...\n",
            "Libro #68898 descargado. 53/200 libros encontrados.\n",
            "Intentando libro #61793...\n",
            "Intentando libro #19193...\n",
            "Libro #19193 descargado. 54/200 libros encontrados.\n",
            "Intentando libro #25739...\n",
            "Libro #25739 descargado. 55/200 libros encontrados.\n",
            "Intentando libro #14512...\n",
            "Intentando libro #46606...\n",
            "Intentando libro #69786...\n",
            "Libro #69786 descargado. 56/200 libros encontrados.\n",
            "Intentando libro #45617...\n",
            "Libro #45617 descargado. 57/200 libros encontrados.\n",
            "Intentando libro #50831...\n",
            "Libro #50831 descargado. 58/200 libros encontrados.\n",
            "Intentando libro #5367...\n",
            "Intentando libro #39541...\n",
            "Intentando libro #66609...\n",
            "Libro #66609 descargado. 59/200 libros encontrados.\n",
            "Intentando libro #45170...\n",
            "Libro #45170 descargado. 60/200 libros encontrados.\n",
            "Intentando libro #13617...\n",
            "Libro #13617 descargado. 61/200 libros encontrados.\n",
            "Intentando libro #15242...\n",
            "Libro #15242 descargado. 62/200 libros encontrados.\n",
            "Intentando libro #29787...\n",
            "Libro #29787 descargado. 63/200 libros encontrados.\n",
            "Intentando libro #55149...\n",
            "Intentando libro #39739...\n",
            "Intentando libro #54931...\n",
            "Libro #54931 descargado. 64/200 libros encontrados.\n",
            "Intentando libro #10707...\n",
            "Intentando libro #37757...\n",
            "Intentando libro #6759...\n",
            "Libro #6759 descargado. 65/200 libros encontrados.\n",
            "Intentando libro #62251...\n",
            "Intentando libro #19067...\n",
            "Intentando libro #12485...\n",
            "Libro #12485 descargado. 66/200 libros encontrados.\n",
            "Intentando libro #52039...\n",
            "Intentando libro #46612...\n",
            "Intentando libro #2575...\n",
            "Libro #2575 descargado. 67/200 libros encontrados.\n",
            "Intentando libro #40511...\n",
            "Libro #40511 descargado. 68/200 libros encontrados.\n",
            "Intentando libro #41504...\n",
            "Intentando libro #21247...\n",
            "Libro #21247 descargado. 69/200 libros encontrados.\n",
            "Intentando libro #2432...\n",
            "Libro #2432 descargado. 70/200 libros encontrados.\n",
            "Intentando libro #54579...\n",
            "Libro #54579 descargado. 71/200 libros encontrados.\n",
            "Intentando libro #8090...\n",
            "Libro #8090 descargado. 72/200 libros encontrados.\n",
            "Intentando libro #17585...\n",
            "Libro #17585 descargado. 73/200 libros encontrados.\n",
            "Intentando libro #40175...\n",
            "Libro #40175 descargado. 74/200 libros encontrados.\n",
            "Intentando libro #59025...\n",
            "Intentando libro #28542...\n",
            "Intentando libro #37025...\n",
            "Intentando libro #21904...\n",
            "Libro #21904 descargado. 75/200 libros encontrados.\n",
            "Intentando libro #48128...\n",
            "Libro #48128 descargado. 76/200 libros encontrados.\n",
            "Intentando libro #37512...\n",
            "Intentando libro #29114...\n",
            "Intentando libro #32738...\n",
            "Libro #32738 descargado. 77/200 libros encontrados.\n",
            "Intentando libro #56089...\n",
            "Libro #56089 descargado. 78/200 libros encontrados.\n",
            "Intentando libro #43131...\n",
            "Libro #43131 descargado. 79/200 libros encontrados.\n",
            "Intentando libro #3921...\n",
            "Intentando libro #38060...\n",
            "Libro #38060 descargado. 80/200 libros encontrados.\n",
            "Intentando libro #15490...\n",
            "Libro #15490 descargado. 81/200 libros encontrados.\n",
            "Intentando libro #16050...\n",
            "Libro #16050 descargado. 82/200 libros encontrados.\n",
            "Intentando libro #41498...\n",
            "Libro #41498 descargado. 83/200 libros encontrados.\n",
            "Intentando libro #6771...\n",
            "Libro #6771 descargado. 84/200 libros encontrados.\n",
            "Intentando libro #51970...\n",
            "Libro #51970 descargado. 85/200 libros encontrados.\n",
            "Intentando libro #67953...\n",
            "Libro #67953 descargado. 86/200 libros encontrados.\n",
            "Intentando libro #32754...\n",
            "Intentando libro #39389...\n",
            "Intentando libro #56931...\n",
            "Intentando libro #52801...\n",
            "Intentando libro #56327...\n",
            "Intentando libro #13407...\n",
            "Libro #13407 descargado. 87/200 libros encontrados.\n",
            "Intentando libro #32298...\n",
            "Intentando libro #49687...\n",
            "Libro #49687 descargado. 88/200 libros encontrados.\n",
            "Intentando libro #63329...\n",
            "Intentando libro #67970...\n",
            "Libro #67970 descargado. 89/200 libros encontrados.\n",
            "Intentando libro #63278...\n",
            "Intentando libro #25923...\n",
            "Libro #25923 descargado. 90/200 libros encontrados.\n",
            "Intentando libro #65063...\n",
            "Intentando libro #25249...\n",
            "Intentando libro #15487...\n",
            "Libro #15487 descargado. 91/200 libros encontrados.\n",
            "Intentando libro #10569...\n",
            "Intentando libro #27233...\n",
            "Libro #27233 descargado. 92/200 libros encontrados.\n",
            "Intentando libro #15482...\n",
            "Libro #15482 descargado. 93/200 libros encontrados.\n",
            "Intentando libro #31599...\n",
            "Intentando libro #43548...\n",
            "Libro #43548 descargado. 94/200 libros encontrados.\n",
            "Intentando libro #30472...\n",
            "Libro #30472 descargado. 95/200 libros encontrados.\n",
            "Intentando libro #12472...\n",
            "Libro #12472 descargado. 96/200 libros encontrados.\n",
            "Intentando libro #48904...\n",
            "Libro #48904 descargado. 97/200 libros encontrados.\n",
            "Intentando libro #61552...\n",
            "Intentando libro #3111...\n",
            "Intentando libro #3599...\n",
            "Intentando libro #31825...\n",
            "Libro #31825 descargado. 98/200 libros encontrados.\n",
            "Intentando libro #26557...\n",
            "Intentando libro #38437...\n",
            "Intentando libro #55402...\n",
            "Libro #55402 descargado. 99/200 libros encontrados.\n",
            "Intentando libro #66502...\n",
            "Intentando libro #67295...\n",
            "Intentando libro #10066...\n",
            "Libro #10066 descargado. 100/200 libros encontrados.\n",
            "Intentando libro #1703...\n",
            "Libro #1703 descargado. 101/200 libros encontrados.\n",
            "Intentando libro #2150...\n",
            "Libro #2150 descargado. 102/200 libros encontrados.\n",
            "Intentando libro #52590...\n",
            "Libro #52590 descargado. 103/200 libros encontrados.\n",
            "Intentando libro #61755...\n",
            "Intentando libro #58752...\n",
            "Libro #58752 descargado. 104/200 libros encontrados.\n",
            "Intentando libro #12731...\n",
            "Libro #12731 descargado. 105/200 libros encontrados.\n",
            "Intentando libro #63619...\n",
            "Libro #63619 descargado. 106/200 libros encontrados.\n",
            "Intentando libro #37953...\n",
            "Libro #37953 descargado. 107/200 libros encontrados.\n",
            "Intentando libro #68517...\n",
            "Libro #68517 descargado. 108/200 libros encontrados.\n",
            "Intentando libro #54619...\n",
            "Libro #54619 descargado. 109/200 libros encontrados.\n",
            "Intentando libro #21195...\n",
            "Intentando libro #9455...\n",
            "Libro #9455 descargado. 110/200 libros encontrados.\n",
            "Intentando libro #67335...\n",
            "Intentando libro #68048...\n",
            "Libro #68048 descargado. 111/200 libros encontrados.\n",
            "Intentando libro #34516...\n",
            "Intentando libro #27405...\n",
            "Intentando libro #42523...\n",
            "Intentando libro #38199...\n",
            "Libro #38199 descargado. 112/200 libros encontrados.\n",
            "Intentando libro #52093...\n",
            "Intentando libro #26907...\n",
            "Libro #26907 descargado. 113/200 libros encontrados.\n",
            "Intentando libro #14547...\n",
            "Intentando libro #69611...\n",
            "Libro #69611 descargado. 114/200 libros encontrados.\n",
            "Intentando libro #42557...\n",
            "Libro #42557 descargado. 115/200 libros encontrados.\n",
            "Intentando libro #65714...\n",
            "Libro #65714 descargado. 116/200 libros encontrados.\n",
            "Intentando libro #10833...\n",
            "Intentando libro #23532...\n",
            "Intentando libro #28162...\n",
            "Libro #28162 descargado. 117/200 libros encontrados.\n",
            "Intentando libro #46028...\n",
            "Libro #46028 descargado. 118/200 libros encontrados.\n",
            "Intentando libro #11427...\n",
            "Libro #11427 descargado. 119/200 libros encontrados.\n",
            "Intentando libro #11961...\n",
            "Libro #11961 descargado. 120/200 libros encontrados.\n",
            "Intentando libro #65452...\n",
            "Intentando libro #26319...\n",
            "Libro #26319 descargado. 121/200 libros encontrados.\n",
            "Intentando libro #44744...\n",
            "Libro #44744 descargado. 122/200 libros encontrados.\n",
            "Intentando libro #6374...\n",
            "Libro #6374 descargado. 123/200 libros encontrados.\n",
            "Intentando libro #64055...\n",
            "Libro #64055 descargado. 124/200 libros encontrados.\n",
            "Intentando libro #40255...\n",
            "Libro #40255 descargado. 125/200 libros encontrados.\n",
            "Intentando libro #33218...\n",
            "Libro #33218 descargado. 126/200 libros encontrados.\n",
            "Intentando libro #10510...\n",
            "Intentando libro #57957...\n",
            "Intentando libro #19675...\n",
            "Intentando libro #46877...\n",
            "Libro #46877 descargado. 127/200 libros encontrados.\n",
            "Intentando libro #44853...\n",
            "Libro #44853 descargado. 128/200 libros encontrados.\n",
            "Intentando libro #37116...\n",
            "Libro #37116 descargado. 129/200 libros encontrados.\n",
            "Intentando libro #38019...\n",
            "Libro #38019 descargado. 130/200 libros encontrados.\n",
            "Intentando libro #69554...\n",
            "Libro #69554 descargado. 131/200 libros encontrados.\n",
            "Intentando libro #23655...\n",
            "Intentando libro #53354...\n",
            "Libro #53354 descargado. 132/200 libros encontrados.\n",
            "Intentando libro #51179...\n",
            "Intentando libro #54137...\n",
            "Libro #54137 descargado. 133/200 libros encontrados.\n",
            "Intentando libro #63229...\n",
            "Intentando libro #26978...\n",
            "Libro #26978 descargado. 134/200 libros encontrados.\n",
            "Intentando libro #52040...\n",
            "Intentando libro #25806...\n",
            "Intentando libro #33121...\n",
            "Libro #33121 descargado. 135/200 libros encontrados.\n",
            "Intentando libro #66143...\n",
            "Intentando libro #11254...\n",
            "Libro #11254 descargado. 136/200 libros encontrados.\n",
            "Intentando libro #9277...\n",
            "Intentando libro #38749...\n",
            "Libro #38749 descargado. 137/200 libros encontrados.\n",
            "Intentando libro #51839...\n",
            "Intentando libro #20631...\n",
            "Intentando libro #14094...\n",
            "Intentando libro #11964...\n",
            "Intentando libro #40862...\n",
            "Libro #40862 descargado. 138/200 libros encontrados.\n",
            "Intentando libro #20933...\n",
            "Libro #20933 descargado. 139/200 libros encontrados.\n",
            "Intentando libro #45120...\n",
            "Intentando libro #3201...\n",
            "Intentando libro #52523...\n",
            "Intentando libro #48024...\n",
            "Libro #48024 descargado. 140/200 libros encontrados.\n",
            "Intentando libro #51242...\n",
            "Intentando libro #39172...\n",
            "Libro #39172 descargado. 141/200 libros encontrados.\n",
            "Intentando libro #54894...\n",
            "Intentando libro #14847...\n",
            "Intentando libro #48152...\n",
            "Libro #48152 descargado. 142/200 libros encontrados.\n",
            "Intentando libro #17813...\n",
            "Libro #17813 descargado. 143/200 libros encontrados.\n",
            "Intentando libro #69667...\n",
            "Intentando libro #40178...\n",
            "Libro #40178 descargado. 144/200 libros encontrados.\n",
            "Intentando libro #2019...\n",
            "Libro #2019 descargado. 145/200 libros encontrados.\n",
            "Intentando libro #49210...\n",
            "Intentando libro #35099...\n",
            "Libro #35099 descargado. 146/200 libros encontrados.\n",
            "Intentando libro #66629...\n",
            "Intentando libro #8810...\n",
            "Intentando libro #45426...\n",
            "Libro #45426 descargado. 147/200 libros encontrados.\n",
            "Intentando libro #31290...\n",
            "Intentando libro #46647...\n",
            "Libro #46647 descargado. 148/200 libros encontrados.\n",
            "Intentando libro #51259...\n",
            "Libro #51259 descargado. 149/200 libros encontrados.\n",
            "Intentando libro #51318...\n",
            "Libro #51318 descargado. 150/200 libros encontrados.\n",
            "Intentando libro #17660...\n",
            "Intentando libro #39925...\n",
            "Libro #39925 descargado. 151/200 libros encontrados.\n",
            "Intentando libro #4151...\n",
            "Intentando libro #12812...\n",
            "Intentando libro #11178...\n",
            "Intentando libro #46013...\n",
            "Libro #46013 descargado. 152/200 libros encontrados.\n",
            "Intentando libro #57071...\n",
            "Libro #57071 descargado. 153/200 libros encontrados.\n",
            "Intentando libro #60854...\n",
            "Intentando libro #39608...\n",
            "Libro #39608 descargado. 154/200 libros encontrados.\n",
            "Intentando libro #40806...\n",
            "Intentando libro #6007...\n",
            "Libro #6007 descargado. 155/200 libros encontrados.\n",
            "Intentando libro #52662...\n",
            "Intentando libro #43566...\n",
            "Intentando libro #2471...\n",
            "Libro #2471 descargado. 156/200 libros encontrados.\n",
            "Intentando libro #49000...\n",
            "Intentando libro #2277...\n",
            "Libro #2277 descargado. 157/200 libros encontrados.\n",
            "Intentando libro #44703...\n",
            "Libro #44703 descargado. 158/200 libros encontrados.\n",
            "Intentando libro #25986...\n",
            "Libro #25986 descargado. 159/200 libros encontrados.\n",
            "Intentando libro #31568...\n",
            "Libro #31568 descargado. 160/200 libros encontrados.\n",
            "Intentando libro #35738...\n",
            "Libro #35738 descargado. 161/200 libros encontrados.\n",
            "Intentando libro #38317...\n",
            "Libro #38317 descargado. 162/200 libros encontrados.\n",
            "Intentando libro #48891...\n",
            "Libro #48891 descargado. 163/200 libros encontrados.\n",
            "Intentando libro #67467...\n",
            "Intentando libro #66193...\n",
            "Libro #66193 descargado. 164/200 libros encontrados.\n",
            "Intentando libro #8645...\n",
            "Libro #8645 descargado. 165/200 libros encontrados.\n",
            "Intentando libro #44870...\n",
            "Intentando libro #15516...\n",
            "Libro #15516 descargado. 166/200 libros encontrados.\n",
            "Intentando libro #21162...\n",
            "Intentando libro #5725...\n",
            "Libro #5725 descargado. 167/200 libros encontrados.\n",
            "Intentando libro #38596...\n",
            "Libro #38596 descargado. 168/200 libros encontrados.\n",
            "Intentando libro #8986...\n",
            "Intentando libro #67825...\n",
            "Libro #67825 descargado. 169/200 libros encontrados.\n",
            "Intentando libro #64439...\n",
            "Intentando libro #53468...\n",
            "Libro #53468 descargado. 170/200 libros encontrados.\n",
            "Intentando libro #35729...\n",
            "Libro #35729 descargado. 171/200 libros encontrados.\n",
            "Intentando libro #34862...\n",
            "Libro #34862 descargado. 172/200 libros encontrados.\n",
            "Intentando libro #27152...\n",
            "Libro #27152 descargado. 173/200 libros encontrados.\n",
            "Intentando libro #63704...\n",
            "Libro #63704 descargado. 174/200 libros encontrados.\n",
            "Intentando libro #43312...\n",
            "Intentando libro #67929...\n",
            "Libro #67929 descargado. 175/200 libros encontrados.\n",
            "Intentando libro #35040...\n",
            "Libro #35040 descargado. 176/200 libros encontrados.\n",
            "Intentando libro #29821...\n",
            "Libro #29821 descargado. 177/200 libros encontrados.\n",
            "Intentando libro #54845...\n",
            "Libro #54845 descargado. 178/200 libros encontrados.\n",
            "Intentando libro #9713...\n",
            "Intentando libro #14778...\n",
            "Libro #14778 descargado. 179/200 libros encontrados.\n",
            "Intentando libro #59339...\n",
            "Intentando libro #35518...\n",
            "Libro #35518 descargado. 180/200 libros encontrados.\n",
            "Intentando libro #19772...\n",
            "Intentando libro #48002...\n",
            "Libro #48002 descargado. 181/200 libros encontrados.\n",
            "Intentando libro #2409...\n",
            "Intentando libro #50906...\n",
            "Libro #50906 descargado. 182/200 libros encontrados.\n",
            "Intentando libro #44274...\n",
            "Libro #44274 descargado. 183/200 libros encontrados.\n",
            "Intentando libro #8606...\n",
            "Libro #8606 descargado. 184/200 libros encontrados.\n",
            "Intentando libro #19906...\n",
            "Libro #19906 descargado. 185/200 libros encontrados.\n",
            "Intentando libro #33335...\n",
            "Libro #33335 descargado. 186/200 libros encontrados.\n",
            "Intentando libro #7403...\n",
            "Libro #7403 descargado. 187/200 libros encontrados.\n",
            "Intentando libro #2180...\n",
            "Libro #2180 descargado. 188/200 libros encontrados.\n",
            "Intentando libro #24341...\n",
            "Intentando libro #44695...\n",
            "Libro #44695 descargado. 189/200 libros encontrados.\n",
            "Intentando libro #35989...\n",
            "Libro #35989 descargado. 190/200 libros encontrados.\n",
            "Intentando libro #13295...\n",
            "Libro #13295 descargado. 191/200 libros encontrados.\n",
            "Intentando libro #55941...\n",
            "Libro #55941 descargado. 192/200 libros encontrados.\n",
            "Intentando libro #31987...\n",
            "Libro #31987 descargado. 193/200 libros encontrados.\n",
            "Intentando libro #48604...\n",
            "Libro #48604 descargado. 194/200 libros encontrados.\n",
            "Intentando libro #50089...\n",
            "Libro #50089 descargado. 195/200 libros encontrados.\n",
            "Intentando libro #14324...\n",
            "Libro #14324 descargado. 196/200 libros encontrados.\n",
            "Intentando libro #12025...\n",
            "Libro #12025 descargado. 197/200 libros encontrados.\n",
            "Intentando libro #21920...\n",
            "Libro #21920 descargado. 198/200 libros encontrados.\n",
            "Intentando libro #21680...\n",
            "Intentando libro #19816...\n",
            "Libro #19816 descargado. 199/200 libros encontrados.\n",
            "Intentando libro #27164...\n",
            "Libro #27164 descargado. 200/200 libros encontrados.\n",
            "Creando corpus combinado para el vocabulario...\n",
            "Creando y adaptando el vocabulario unificado...\n",
            "Creando datasets de pre-entrenamiento y fine-tuning...\n",
            "✅ Preparación por palabras unificada completada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 5. ETAPA DE PRE-ENTRENAMIENTO\n",
        "# ==============================================================================\n",
        "print(\"--- Iniciando Etapa de Pre-entrenamiento (Palabras) ---\")\n",
        "\n",
        "embedding_dim = 512\n",
        "num_heads = 8\n",
        "feed_forward_dim = 2048\n",
        "num_transformer_blocks = 4\n",
        "\n",
        "transformer_model = TransformerTextModel(\n",
        "    vocab_size=len(vocabulary),\n",
        "    embedding_dim=embedding_dim,\n",
        "    num_heads=num_heads,\n",
        "    feed_forward_dim=feed_forward_dim,\n",
        "    num_transformer_blocks=num_transformer_blocks\n",
        ")\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "lr_schedule = CustomSchedule(embedding_dim)\n",
        "optimizer_pre = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "transformer_model.compile(optimizer=optimizer_pre, loss=loss)\n",
        "\n",
        "epocas_preentrenamiento = 100\n",
        "transformer_model.fit(dataset_preentrenamiento, epochs=epocas_preentrenamiento, verbose=1)\n",
        "\n",
        "transformer_model.save_weights(path + \"word_unified_pretrained.weights.h5\")\n",
        "print(\"Pre-entrenamiento completado.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "ZHP-RdcZJLeH",
        "outputId": "def0e35a-c804-4601-e229-c38976d523fa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Iniciando Etapa de Pre-entrenamiento (Palabras) ---\n",
            "Epoch 1/100\n",
            "\u001b[1m1114/1114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2357s\u001b[0m 2s/step - loss: 7.6749\n",
            "Epoch 2/100\n",
            "\u001b[1m1114/1114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2350s\u001b[0m 2s/step - loss: 4.6848\n",
            "Epoch 3/100\n",
            "\u001b[1m1114/1114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2351s\u001b[0m 2s/step - loss: 4.3013\n",
            "Epoch 4/100\n",
            "\u001b[1m1114/1114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2350s\u001b[0m 2s/step - loss: 4.1179\n",
            "Epoch 5/100\n",
            "\u001b[1m1114/1114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2347s\u001b[0m 2s/step - loss: 3.9549\n",
            "Epoch 6/100\n",
            "\u001b[1m1114/1114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2346s\u001b[0m 2s/step - loss: 3.7831\n",
            "Epoch 7/100\n",
            "\u001b[1m1114/1114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2349s\u001b[0m 2s/step - loss: 3.6417\n",
            "Epoch 8/100\n",
            "\u001b[1m1114/1114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2353s\u001b[0m 2s/step - loss: 3.5130\n",
            "Epoch 9/100\n",
            "\u001b[1m 351/1114\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m26:53\u001b[0m 2s/step - loss: 3.4254"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-293976731.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mepocas_preentrenamiento\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtransformer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_preentrenamiento\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepocas_preentrenamiento\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtransformer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"word_unified_pretrained.weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# 6. ETAPA DE FINE-TUNING\n",
        "# ==============================================================================\n",
        "print(\"--- Iniciando Etapa de Fine-Tuning (Palabras) ---\")\n",
        "\n",
        "for inputs, targets in dataset_finetuning.take(1):\n",
        "    _ = transformer_model(inputs)\n",
        "transformer_model.load_weights(path + \"word_unified_pretrained.weights.h5\")\n",
        "\n",
        "optimizer_fine = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "transformer_model.compile(optimizer=optimizer_fine, loss=loss)\n",
        "\n",
        "epocas_finetuning = 20\n",
        "transformer_model.fit(dataset_finetuning, epochs=epocas_finetuning, verbose=1)\n",
        "\n",
        "transformer_model.save_weights(path + \"word_unified_finetuned.weights.h5\")\n",
        "print(\"✅ Fine-Tuning completado.\")"
      ],
      "metadata": {
        "id": "1Q376Z_kX94a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 7. GENERACIÓN DE TEXTO CON EL MODELO FINE-TUNED\n",
        "# ==============================================================================\n",
        "\n",
        "class TextGenerator:\n",
        "    def __init__(self, model, vectorize_layer, vocabulary, temperature=1.0):\n",
        "        self.model = model\n",
        "        self.vectorize_layer = vectorize_layer\n",
        "        self.temperature = temperature\n",
        "        self.id_to_word_table = tf.constant(vocabulary)\n",
        "\n",
        "    def generate(self, prompt, max_length=200):\n",
        "        processed_prompt = preprocess_text(prompt)\n",
        "        input_ids = self.vectorize_layer([processed_prompt])\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            predictions = self.model(input_ids)\n",
        "            last_token_logits = predictions[:, -1, :]\n",
        "            scaled_logits = last_token_logits / self.temperature\n",
        "            new_token_id = tf.random.categorical(scaled_logits, num_samples=1)\n",
        "            input_ids = tf.concat([input_ids, new_token_id], axis=1)\n",
        "\n",
        "        output_words = tf.gather(self.id_to_word_table, input_ids)\n",
        "\n",
        "\n",
        "        generated_text = tf.strings.reduce_join(output_words, axis=-1, separator=' ').numpy()[0].decode('utf-8')\n",
        "\n",
        "        return postprocess_text(generated_text)\n",
        "\n",
        "\n",
        "print(\"\\n--- Iniciando Generación de Texto ---\")\n",
        "\n",
        "# 1. Crea una instancia del generador\n",
        "text_generator = TextGenerator(\n",
        "    model=transformer_model,\n",
        "    vectorize_layer=vectorize_layer,\n",
        "    vocabulary=vocabulary,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# 2. prompt\n",
        "prompt_inicial = \"The world seemed like such a peaceful place until the magic tree was discovered in London.\"\n",
        "\n",
        "# 3. Genera el texto\n",
        "texto_generado = text_generator.generate(prompt_inicial, max_length=250)\n",
        "\n",
        "print(f\"Prompt: {prompt_inicial}\\n\")\n",
        "print(\"Texto Generado:\")\n",
        "print(texto_generado)"
      ],
      "metadata": {
        "id": "XqfcwtwfBr0T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}